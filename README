This README file discusses the Python code associated with the paper:

Ryan Prescott Adams, Zoubin Ghahramani and Michael I. Jordan.
Tree-Structured Stick Breaking for Hierarchical Data.  Advances in Neural
Information Processing Systems (NIPS) 23.  J. Shawe-Taylor, R. Zemel,
J. Lafferty, and C. Williams, eds.  2010.

There are two types of data analyzed in the paper: images and text.  The
image data analyzed are the CIFAR-100 data, available at
http://www.cs.utoronto.ca/~kriz/cifar.html.  

Note that this is research-quality software and I cannot guarantee that it
will work on your system and your configuration.  I make this available
simply so that you can see how the guts of this worked for the analysis I
did in the paper.  Your mileage will vary.

You can run the image clustering task by simply running "python
cifar100-50k.py". It will use the "random-word" program to give your
results a unique name.  It will then load things up and start the MCMC,
producing checkpoint files in the "checkpoints" folder, with the current
best in the "bests" folder.  The idea is that you can look at the trees
associated with these Python pickles by turning them into special files
that are interpretable by a piece of commercial graph visualization
software called aiSee (http://www.aisee.com/).  Running the
"cifar100-graph.py" script with one of the pickle files as its output will
produce a file with a ".gdl" file extension that can be loaded by aiSee.
For this to look pretty, you will need to have all of the actual PNG images
available on your machine, and you'll need to modify the paths
appropriately.  I have made the PNGs available at
http://www.cs.toronto.edu/~rpa/adams-ghahramani-jordan-2010a.shtml.

To run the topic modeling code, you will need more than just Python.  You
will also need Cython (http://cython.org/), Cudamat
(http://code.google.com/p/cudamat/), and GPUnumpy
(http://www.cs.toronto.edu/~tijmen/gnumpy.html).  You'll need the
nips_1-12.mat file from http://www.cs.nyu.edu/~roweis/data.html.  You will
need to run the "./build.sh" script, modifying the paths so that it matches
your installation of Python and Numpy.  This will compile the C that makes
LDA-related things run faster.  You can then run the vanilla LDA stuff with
"python run_nips_lda.py XXX", where "XXX" is the number of topics you want
it to use, or the TSSB stuff with "python run_nips_tssb.py XXX".  This will
run a bunch of jobs and create directories with results.  After this,
you'll need to do the actual evaluation, which is GPU-acclerated, and uses
the eval_nips_*.py files.  These will print out the numeric results.

Good luck.  Python is great, but Free Software developers are always
changing their APIs, so this I assume this code will be broken for most
later versions of the third-party libraries.  Hopefully you can still see
how it all works.
